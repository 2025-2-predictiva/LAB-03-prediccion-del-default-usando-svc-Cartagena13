{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c59570",
   "metadata": {},
   "source": [
    "# Notebook: Entrenamiento paso a paso del modelo (homework)\n",
    "\n",
    "\n",
    "- Lectura de datos\n",
    "- Limpieza y transformaciones\n",
    "- Separación en entrenamiento/prueba\n",
    "- Construcción del pipeline (OneHotEncoder + RandomForest)\n",
    "- Búsqueda de hiperparámetros (GridSearchCV)\n",
    "- Persistencia del modelo y exportación de métricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9513a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports y configuración básica\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cb7083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas (nombres ligeramente cambiados para variar la redacción)\n",
    "MODEL_FILEPATH = \"../files/models/model.pkl.gz\"\n",
    "METRICS_FILEPATH = \"../files/output/metrics.json\"\n",
    "TRAIN_FILE = os.path.join(\"..\",\"files\", \"input\", \"train_data.csv.zip\")\n",
    "TEST_FILE = os.path.join(\"..\",\"files\", \"input\", \"test_data.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21a3805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_files() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Carga los CSV comprimidos (train/test) y devuelve DataFrames.\"\"\"\n",
    "    if not os.path.exists(TRAIN_FILE):\n",
    "        raise FileNotFoundError(f\"No encuentro {TRAIN_FILE}\")\n",
    "    if not os.path.exists(TEST_FILE):\n",
    "        raise FileNotFoundError(f\"No encuentro {TEST_FILE}\")\n",
    "\n",
    "    # pandas detecta la compresión por la extensión .zip\n",
    "    train_df = pd.read_csv(TRAIN_FILE)\n",
    "    test_df = pd.read_csv(TEST_FILE)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92aeaf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_df(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aplica transformaciones iniciales al DataFrame (limpieza ligera).\"\"\"\n",
    "    # Renombrar la columna objetivo si viene con otro nombre\n",
    "    if \"default payment next month\" in data.columns:\n",
    "        data = data.rename(columns={\"default payment next month\": \"default\"})\n",
    "\n",
    "    # Eliminar identificador si está presente\n",
    "    if \"ID\" in data.columns:\n",
    "        data = data.drop(columns=[\"ID\"])\n",
    "\n",
    "    # Normalizar valores de EDUCATION mayores a 4 hacia la categoría 'otros' (4)\n",
    "    if \"EDUCATION\" in data.columns:\n",
    "        data.loc[data[\"EDUCATION\"] > 4, \"EDUCATION\"] = 4\n",
    "\n",
    "    # Eliminar filas con NA y reindexar\n",
    "    cleaned = data.dropna(axis=0, how=\"any\").reset_index(drop=True)\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98bbd160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features_target(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Devuelve (features, target). Valida que exista la columna objetivo.\"\"\"\n",
    "    if \"default\" not in df.columns:\n",
    "        raise RuntimeError('La columna objetivo \"default\" no existe en el dataset.')\n",
    "\n",
    "    target = df[\"default\"].astype(int)\n",
    "    features = df.drop(columns=[\"default\"])\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c49b9f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42  # si ya lo tienes definido, usa el mismo\n",
    "\n",
    "def make_model_search(X_train: pd.DataFrame) -> GridSearchCV:\n",
    "    \"\"\"\n",
    "    Pipeline pedido en el lab 3:\n",
    "    - OneHot para categóricas\n",
    "    - PCA (todas las componentes)\n",
    "    - Estandarizar\n",
    "    - SelectKBest\n",
    "    - SVM\n",
    "    \"\"\"\n",
    "\n",
    "    # Categóricas: las de antes + estados de pago (son categorías discretas)\n",
    "    cat_cols = [\n",
    "        col\n",
    "        for col in [\n",
    "            \"SEX\",\n",
    "            \"EDUCATION\",\n",
    "            \"MARRIAGE\",\n",
    "            \"PAY_0\",\n",
    "            \"PAY_2\",\n",
    "            \"PAY_3\",\n",
    "            \"PAY_4\",\n",
    "            \"PAY_5\",\n",
    "            \"PAY_6\",\n",
    "        ]\n",
    "        if col in X_train.columns\n",
    "    ]\n",
    "\n",
    "    # Numéricas: todo lo demás\n",
    "    num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "    # Paso 1: One-Hot para categóricas, pasar numéricas crudas\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "            (\"num\", \"passthrough\", num_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Clasificador SVM\n",
    "    svm_clf = SVC(random_state=RANDOM_STATE)\n",
    "\n",
    "    # Pipeline en el orden que pide el enunciado:\n",
    "    # OHE -> PCA -> StandardScaler -> SelectKBest -> SVM\n",
    "    pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"pca\", PCA()),  # usa todas las componentes\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"feature_selection\", SelectKBest(score_func=f_classif, k=20)),\n",
    "            (\"classifier\", svm_clf),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Grid de hiperparámetros para CV\n",
    "    svm_clf = SVC(kernel=\"rbf\")  # fijamos el kernel\n",
    "\n",
    "    param_grid = {\n",
    "    \"feature_selection__k\": [10, 20, 30],   # dos opciones de K\n",
    "    \"classifier__C\": [1.0, 5.0, 10.0],       # dos opciones de C\n",
    "    # sin gamma en el grid: usamos el default \"scale\"\n",
    "    }\n",
    "    \n",
    "\n",
    "    search = GridSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_grid=param_grid,\n",
    "        cv=10,  \n",
    "        scoring=\"balanced_accuracy\",\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "    )\n",
    "\n",
    "    return search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "612348d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. MÉTRICAS\n",
    "# ---------------------------------------------------------------------\n",
    "def build_classification_metrics(y_true, y_pred, dataset_name: str) -> dict:\n",
    "    \"\"\"Genera el dict de métricas (misma salida que antes, solo renombrado).\"\"\"\n",
    "    return {\n",
    "        \"type\": \"metrics\",\n",
    "        \"dataset\": dataset_name,\n",
    "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        \"balanced_accuracy\": float(balanced_accuracy_score(y_true, y_pred)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "        \"f1_score\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "    }\n",
    "\n",
    "\n",
    "def build_confusion_matrix_record(y_true, y_pred, dataset_name: str) -> dict:\n",
    "    \"\"\"Devuelve la matriz de confusión como diccionario (mismo formato que antes).\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    return {\n",
    "        \"type\": \"cm_matrix\",\n",
    "        \"dataset\": dataset_name,\n",
    "        \"true_0\": {\"predicted_0\": int(tn), \"predicted_1\": int(fp)},\n",
    "        \"true_1\": {\"predicted_0\": int(fn), \"predicted_1\": int(tp)},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8112f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. GUARDAR MODELO Y MÉTRICAS\n",
    "# ---------------------------------------------------------------------\n",
    "def persist_model(model) -> None:\n",
    "    os.makedirs(os.path.dirname(MODEL_FILEPATH), exist_ok=True)\n",
    "    with gzip.open(MODEL_FILEPATH, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "\n",
    "def persist_metrics(records) -> None:\n",
    "    \"\"\"\n",
    "    records: lista de diccionarios.\n",
    "    Orden esperado por el test:\n",
    "      0 -> métricas train\n",
    "      1 -> métricas test\n",
    "      2 -> matriz confusión train\n",
    "      3 -> matriz confusión test\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(METRICS_FILEPATH), exist_ok=True)\n",
    "    with open(METRICS_FILEPATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in records:\n",
    "            f.write(json.dumps(rec) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f11e6c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. MAIN\n",
    "# ---------------------------------------------------------------------\n",
    "def run_workflow() -> None:\n",
    "    # Paso 1: cargar y limpiar\n",
    "    raw_train, raw_test = read_raw_files()\n",
    "    train_df = sanitize_df(raw_train)\n",
    "    test_df = sanitize_df(raw_test)\n",
    "\n",
    "    # Paso 2: separar en features/target\n",
    "    x_train, y_train = split_features_target(train_df)\n",
    "    x_test, y_test = split_features_target(test_df)\n",
    "\n",
    "    # Pasos 3 y 4: pipeline y búsqueda de hiperparámetros\n",
    "    model_search = make_model_search(x_train)\n",
    "    model_search.fit(x_train, y_train)\n",
    "\n",
    "    # Paso 5: persistir el modelo encontrado\n",
    "    persist_model(model_search)\n",
    "\n",
    "    # Paso 6: predicciones y cálculo de métricas\n",
    "    y_pred_train = model_search.predict(x_train)\n",
    "    y_pred_test = model_search.predict(x_test)\n",
    "\n",
    "    metrics_train = build_classification_metrics(y_train, y_pred_train, \"train\")\n",
    "    metrics_test = build_classification_metrics(y_test, y_pred_test, \"test\")\n",
    "\n",
    "    # Paso 7: matrices de confusión\n",
    "    cm_train = build_confusion_matrix_record(y_train, y_pred_train, \"train\")\n",
    "    cm_test = build_confusion_matrix_record(y_test, y_pred_test, \"test\")\n",
    "\n",
    "    # Guardar todas las métricas en el archivo de salida\n",
    "    persist_metrics([metrics_train, metrics_test, cm_train, cm_test])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_workflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
